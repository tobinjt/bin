#!/bin/bash

set -e -f -u -o pipefail

create_datebase() {
  local database="$1"
  # The schema is taken from /usr/share/doc/linkchecker/examples/create.sql and
  # the timestamp column added.
  sqlite3 "${database}" <<END_OF_CREATE
CREATE TABLE linksdb (
  urlname       VARCHAR(256) NOT NULL,
  parentname    VARCHAR(256),
  baseref       VARCHAR(256),
  valid         INT,
  result        VARCHAR(256),
  warning       VARCHAR(512),
  info          VARCHAR(512),
  url           VARCHAR(256),
  line          INT,
  col           INT,
  name          VARCHAR(256),
  checktime     INT,
  dltime        INT,
  size          INT,
  cached        INT,
  level         INT NOT NULL,
  modified      VARCHAR(256),
  timestamp     DATETIME DEFAULT CURRENT_TIMESTAMP
);
END_OF_CREATE
}

run_linkchecker() {
  local url="$1" hostname="$2"
  # - Cut down on output so this can be run from cron, but write to a file that
  #   can be displayed on failure or when run interactively.
  # - Use a small threadpool to hopefully reduce false positive failures.
  # - Check external links but do not recurse on external sites.
  # - stderr will frequently have a warning about SSL certificates.
  # - To figure out problems run with '--debug=checking' and look for the URLs
  #   that keep getting checked at the end.
  linkchecker \
    --no-status \
    --quiet \
    --file-output=text \
    --file-output=sql \
    --threads=10 \
    --check-extern \
    --no-follow-url="!://${hostname}/" \
    "${url}" > stdout 2> stderr
}

report_persistently_bad_urls() {
  local database="$1" date_threshold="$2" count_threshold="$3"
  sqlite3 "${database}" << END_OF_QUERY
CREATE TEMP TABLE url_counts
  AS SELECT urlname, COUNT(urlname) AS counts
    FROM linksdb
    WHERE timestamp > '${date_threshold}'
    GROUP BY urlname;

SELECT urlname, counts
  FROM url_counts
  WHERE counts >= ${count_threshold};
END_OF_QUERY
}

check_one_website() {
  local url="$1" hostname
  hostname="$(sed -e 's,https\?://,,' -e 's,/.*$,,' <<<"${url}")"
  local database="${HOME}/tmp/linkchecker-cron.${hostname}.sqlite3"
  if [[ ! -f "${database}" ]]; then
    create_datebase "${database}"
  fi
  mkdir -p "${hostname}"
  cd "${hostname}"

  if ! run_linkchecker "${url}" "${hostname}"; then
    sqlite3 "${database}" < linkchecker-out.sql
    local output date_threshold count_threshold=5
    date_threshold="$(date --date '2 days ago' +%Y-%m-%d)"
    output="$(report_persistently_bad_urls "${database}" "${date_threshold}" \
                "${count_threshold}")"
    if [[ -n "${output}" ]]; then
      printf "Bad URLs for %s since %s\\n" \
        "${url}" "${date_threshold}"
      awk -F '|' '{print $2, $1}' <<<"${output}"
      return 1
    fi
  fi
  return 0
}

main() {
  if [[ ! -t 0 ]]; then
    sleep $((RANDOM % 600))
  fi
  local destdir url exit_status=0
  destdir="$(mktemp -d -t linkchecker-cron.XXXXXXXXXX)"
  for url in "$@"; do
    cd "${destdir}"
    check_one_website "${url}" || exit_status="$?"
  done

  if [[ -t 0 || "${exit_status}" != 0 ]]; then
    printf "Output in %s\\n" "${destdir}"
  else
    rm -rf "${destdir}"
  fi
  return "${exit_status}"
}

main "$@"

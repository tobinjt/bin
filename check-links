#!/bin/bash

set -e -u -f -o pipefail

main() {
  WEBSITE="$1"
  DIR="$(mktemp -d /tmp/check-links-XXXXXXXXXX)"
  LOG="${DIR}/wget.log"
  STATUS_FILE="${DIR}/request_status"
  readonly WEBSITE DIR LOG STATUS_FILE
  cd "${DIR}"

  reject_regex="/xmlrpc.php|/blog/Smarter_HTTP_redirects/"
  wget \
    --output-file wget.log \
    --execute robots=off \
    --reject-regex "${reject_regex}" \
    --content-on-error \
    --wait 1 \
    --recursive \
    --level 10 \
    --page-requisites \
    "${WEBSITE}" || true

  # -- marks the start of a request; replace the hold space with that line.
  # When the request status is seen, append the line to the hold space, swap the
  # hold and pattern spaces, replace newline with space to join the lines, then
  # print the pattern space.
  sed -n \
      -e '/^--/ h' \
      -e '/HTTP request sent, awaiting response.../ { H; g; s/\n/ /; p; }' \
      "${LOG}" > "${STATUS_FILE}"
  PROBLEMS="$(awk '
  # Save URLs into either GOOD_URLS or BAD_URLS.
  {
    if (match($0, "200 OK$")) {
      for (i = 1; i < NF; i++) {
        if (match($i, "https?://")) {
          GOOD_URLS[$i] = $0;
        }
      }
    } else {
      for (i = 1; i < NF; i++) {
        if (match($i, "https?://")) {
          BAD_URLS[$i] = $0;
        }
      }
    }
  }
  # Check every URL in BAD_URLS; if it is found in GOOD_URLS a retry was
  # successful, otherwise print it.
  END {
    for (url in BAD_URLS) {
      if (url in GOOD_URLS == 0) {
        print BAD_URLS[url];
      }
    }
  }' "${STATUS_FILE}")"
  # Check for common error messages too.
  PROBLEMS+="$(grep -r -F -e 'Fatal error' "${DIR}" || true)"

  if [[ -n "${PROBLEMS}" || -t 0 ]]; then
    if [[ -n "${PROBLEMS}" ]]; then
      echo "Problems found:"
      echo "${PROBLEMS}"
    else
      echo "No problems found :)"
    fi
    echo "See ${LOG} and the contents of ${DIR} for further investigation"
  else
    rm -rf "${DIR}"
  fi
}

# Only run main if being executed directly; do nothing if sourced for testing.
# Use basename so that it can be run as 'bash -x ./check-links ...'.
if [[ "$(basename "${BASH_SOURCE[0]}")" == "$(basename "${0}")" ]]; then
  if [ "$#" != 1 ] || [ "${1:-}" == '-h' ] || [ "${1:-}" == '--help' ]; then
    echo "Usage: $0 http://SOME-WEBSITE/" >&2
    echo "Links outside SOME-WEBSITE will not be checked." >&2
    echo "Page contents will be downloaded and saved for investigation." >&2
    exit 1
  fi

  if [[ ! -t 0 ]]; then
    sleep $((RANDOM % 600))
  fi

  # Needed for wget on Mac OS.
  PATH="${PATH}:/usr/local/bin"
  main "$@"
fi
